{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7qta-cE7KRg"
      },
      "source": [
        "\n",
        "\n",
        "# **Special Matrices for Equity Analysis**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mCjn75M80eS"
      },
      "source": [
        "## **1. Symmetric Matrices**\n",
        "### **1.1. Definition of Symmetric Matrices**\n",
        "A **symmetric matrix** is a square matrix whose transpose is the same as the matrix itself. A square matrix is a matrix that has the same number of rows as the number of columns. The following example demonstrates the concept of a symmetric matrix.\n",
        "<br>\n",
        "<br>\n",
        "$$A= \\begin{bmatrix}\n",
        "m & a & b \\\\\n",
        "a & n & c \\\\\n",
        "b & c & p\n",
        "\\end{bmatrix}$$\n",
        "<br>\n",
        "<br>\n",
        "$$A^{T}= \\begin{bmatrix}\n",
        "m & a & b \\\\\n",
        "a & n & c \\\\\n",
        "b & c & p\n",
        "\\end{bmatrix}$$\n",
        "<br>\n",
        "<br>\n",
        "In the above example, the second matrix is the transpose of $A$ matrix. The elements of the original matrix and its transpose are the same. In mathematical notation, for matrix $A$, if $A = A^{T}$, then $A$ is symmetric. To be more specific, the element at position (i,j) is equal to the element at position (j,i) for all i and j.\n",
        "<br>\n",
        "Here are some key properties of a symmetric matrix:\n",
        "<br>\n",
        "<br>\n",
        "&emsp;&emsp;1. The eigenvalues of the matrix are real numbers.\n",
        "<br>\n",
        "&emsp;&emsp;2. The eigenvectors of the matrix are orthogonal to each other.\n",
        "<br>\n",
        "&emsp;&emsp;3. The matrix is diagonalizable.\n",
        "<br>\n",
        "&emsp;&emsp;4. The rank of the matrix is equal to the number of non-zero eigenvalues.\n",
        "<br>\n",
        "<br>\n",
        "When two vectors are **orthogonal**, it means that the vectors are perpendicular to each other. It also means the inner product of the two vectors is 0. These two vectors are linearly independent. Next, we will talk about the diagonalizability of a symmetric matrix.\n",
        "<br>\n",
        "<br>\n",
        "### **1.2. Diagonalization of Symmetric Matrices**\n",
        "Matrix $A$ is diagonalizable if there exists a diagonal matrix $\\Lambda$ such that\n",
        "<br>\n",
        "<br>\n",
        "$$A=B\\Lambda B^{-1}$$\n",
        "<br>\n",
        "If $A$ is a symmetric matrix, the values on the main diagonal of $\\Lambda$ are eigenvalues of $A$. $B$ is a matrix whose columns are eigenvectors of $A$.\n",
        "<br>\n",
        "Let's look at one example of diagonalization of a symmetric matrix.\n",
        "<br>\n",
        "We have a symmetric matrix $M$ as follows:\n",
        "<br>\n",
        "<br>\n",
        "$$M = \\begin{bmatrix}\n",
        "6 & -2 & -1 \\\\\n",
        "-2 & 6 & -1 \\\\\n",
        "-1 & -1 & 5\n",
        "\\end{bmatrix}$$\n",
        "<br>\n",
        "<br>\n",
        "In order to obtain eigenvalues, we need to set the characteristic polynomial to 0, or solve for the characteristic equation as follows:\n",
        "<br>\n",
        "<br>\n",
        "$$det(M-\\lambda I) = det(\\begin{bmatrix}\n",
        "6-\\lambda & -2 & -1 \\\\\n",
        "-2 & 6-\\lambda & -1 \\\\\n",
        "-1 & -1 & 5-\\lambda\n",
        "\\end{bmatrix})$$\n",
        "<br>\n",
        "$$\\Downarrow $$\n",
        "<br>\n",
        "$$(6-\\lambda)(6-\\lambda)(5-\\lambda)+(-2)(-1)(-1)+(-2)(-1)(-1)-(-1)(-1)(6-\\lambda)-(-2)(-2)(5-\\lambda)-(-1)(-1)(6-\\lambda)=0$$\n",
        "<br>\n",
        "$$\\Downarrow $$\n",
        "<br>\n",
        "$$-\\lambda^{3}+17\\lambda^{2}-90\\lambda+144=-(\\lambda-8)(\\lambda-6)(\\lambda-3)=0$$\n",
        "<br>\n",
        "<br>\n",
        "Hence, the eigenvalues are $\\lambda_{1}=8$, $\\lambda_{2}=6$ and $\\lambda_{3}=3$.\n",
        "The corresponding eigenvectors are as follows:\n",
        "<br>\n",
        "<br>\n",
        "$$\\nu_{1}=\\begin{bmatrix}\n",
        "-1 \\\\\n",
        " 1\\\\\n",
        "0\n",
        "\\end{bmatrix}, \\nu_{2}=\\begin{bmatrix}\n",
        "-1 \\\\\n",
        "-1\\\\\n",
        "2\n",
        "\\end{bmatrix}, \\nu_{3}=\\begin{bmatrix}\n",
        "1 \\\\\n",
        "1\\\\\n",
        "1\n",
        "\\end{bmatrix}$$\n",
        "<br>\n",
        "<br>\n",
        "One thing to pay attention to is that the inner products of each eigenvector pairs are all 0 ($\\nu_{1}^{T}\\nu_{2}=0,\\nu_{2}^{T}\\nu_{3}=0,\\nu_{1}^{T}\\nu_{3}=0$). This means that they are orthogonal to each other.\n",
        "<br>\n",
        "<br>\n",
        "We can further normalize eigenvectors to be unit vectors as follows:\n",
        "<br>\n",
        "<br>\n",
        "$$\\omega_{1}=\\begin{bmatrix}\n",
        "-\\frac{1}{\\sqrt{2}} \\\\\n",
        " \\frac{1}{\\sqrt{2}}\\\\\n",
        "0\n",
        "\\end{bmatrix}, \\omega_{2}=\\begin{bmatrix}\n",
        "-\\frac{1}{\\sqrt{6}} \\\\\n",
        "- \\frac{1}{\\sqrt{6}}\\\\\n",
        "\\frac{2}{\\sqrt{6}}\n",
        "\\end{bmatrix}, \\omega_{3}=\\begin{bmatrix}\n",
        "\\frac{1}{\\sqrt{3}} \\\\\n",
        "\\frac{1}{\\sqrt{3}}\\\\\n",
        "\\frac{1}{\\sqrt{3}}\n",
        "\\end{bmatrix}$$\n",
        "<br>\n",
        "<br>\n",
        "Now we can write down the diagonalized symmetric matrix $M$.\n",
        "<br>\n",
        "<br>\n",
        "$$M=B\\Lambda B^{-1}$$\n",
        "<br>\n",
        "where\n",
        "$$\\Lambda=\\begin{bmatrix}\n",
        "8 & 0 & 0 \\\\\n",
        "0 & 6 & 0 \\\\\n",
        "0 & 0 & 3\n",
        "\\end{bmatrix}$$\n",
        "<br>\n",
        "$$B=\\begin{bmatrix}\n",
        "-\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} \\\\\n",
        " \\frac{1}{\\sqrt{2}}& -\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} \\\\\n",
        " 0& \\frac{2}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}}\n",
        "\\end{bmatrix}$$\n",
        "<br>\n",
        "<br>\n",
        "Because $B$ is an orthogonal matrix, $B^{T}=B^{-1}$. We can also write the diagonalized symmetric matrix as follows:\n",
        "<br>\n",
        "<br>\n",
        "$$M=B\\Lambda B^{T}$$\n",
        "<br>\n",
        "<br>\n",
        "Transforming a symmetric matrix to the diagonalized form will make the computation of matrices more efficient. It also helps decompose a matrix into simpler forms for analysis. Diagonalization of a matrix is also an essential tool to help solve the linear transformation issue.\n",
        "<br>\n",
        "Now we have learned many properties of symmetric matrices. Let's take a look at some common symmetric matrices, like the covariance matrix and the correlation matrix. We'll look at the covariance matrix first.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cllN3_qk-PC1"
      },
      "source": [
        "### **1.3. Covariance Matrix**\n",
        "We introduced the covariance matrix in the previous lesson. A covariance matrix is a square matrix giving the variances of the variables on the diagonal of the matrix and the covariances between each pair of the variables on the off-diagonal. Based on the structure of a covariance matrix, it is a symmetric matrix. Let's calculate the covariance matrix for stock returns for Apple, Ford Motor, and Walmart as an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwinNmSX-eou"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yfin\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRdMAtIm-j8i"
      },
      "outputs": [],
      "source": [
        "# Download stock prices from Yahoo Finance and set the time period for download\n",
        "start = datetime.date(2018, 1, 2)\n",
        "end = datetime.date(2023, 12, 31)\n",
        "stocks = yfin.download([\"AAPL\", \"F\", \"WMT\"], start, end, auto_adjust = False)[\"Adj Close\"]\n",
        "stocks.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCgk1N-uGZ1Y"
      },
      "outputs": [],
      "source": [
        "stocks.index = pd.to_datetime(stocks.index).strftime(\"%Y-%m-%d\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKNn4jDZKAB9"
      },
      "outputs": [],
      "source": [
        "# In order to calculate the returns of the stocks, we need to drop the NA rows.\n",
        "stocks_returns = stocks[[\"AAPL\", \"F\", \"WMT\"]].dropna().pct_change()\n",
        "stocks_returns = stocks_returns.dropna()\n",
        "stocks_returns.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWuKTnDeNdb4"
      },
      "source": [
        "With the stock returns for Apple, Ford Motor and Walmart, we can calculate the covariance matrix for the 3 stock returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08kUMTh0NpiR"
      },
      "outputs": [],
      "source": [
        "# Calculate covariance matrix for Apple, Ford Motor and Walmart\n",
        "# Use np.allclose to verify if the matrix is a symmetric matrix\n",
        "stock_returns_covariance_matrix = stocks_returns.cov()\n",
        "print(\"Apple, Ford Motor and Walmart Covariance Matrix:\")\n",
        "print(stock_returns_covariance_matrix)\n",
        "print(\"\\nIs the covariance matrix symmetric?\", np.allclose(stock_returns_covariance_matrix, stock_returns_covariance_matrix.T))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyLCCyEJOi5n"
      },
      "source": [
        "As we can see from the above covariance matrix for Apple, Ford Motor, and Walmart, the variances of the three stocks are on the diagonal of the matrix. The covariances of any pairs of the stocks are on the off-diagonal of the matrix. This covariance matrix is also a symmetric matrix based on the definition.\n",
        "<br>\n",
        "<br>\n",
        "### **1.4. Correlation Matrix**\n",
        "According to the previous lesson, correlation is a metric to measure the co-movement of two variables while removing their scales. Without the scale difference, we can compare correlations from different pairs of variables. It is also a symmetric matrix. Let's continue our Apple, Ford Motor, and Walmart stock return example to calculate their correlation matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_4dk7AQSH97"
      },
      "outputs": [],
      "source": [
        "# Calculate correlation matrix for Apple, Ford Motor and Walmart\n",
        "# Verify if a correlation matrix a symmetric matrix\n",
        "stock_returns_correlation_matrix = stocks_returns.corr()\n",
        "print(\"Apple, Ford Motor and Walmart Correlation Matrix:\")\n",
        "print(stock_returns_correlation_matrix)\n",
        "print(\"\\nIs the correlation matrix symmetric?\", np.allclose(stock_returns_correlation_matrix, stock_returns_correlation_matrix.T))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWBgi-gmSgQg"
      },
      "source": [
        "We can see from the above correlation matrix that it is also a symmetric matrix. The diagonal values are the correlations of 3 stocks to themselves. The off-diagonal values are the correlations of different paired stock returns. Apple's stock return has similar correlation to both Ford Motor and Walmart's stock returns. Ford Motor and Walmart's stock returns have the lowest correlation among other stock return pairs.\n",
        "<br>\n",
        "<br>\n",
        "### **1.5. Converting between Covariance and Correlation Matrices**\n",
        "From the previous lesson, we learned that covariance and correlation have the following relationship.\n",
        "<br>\n",
        "<br>\n",
        "$$Corr(𝑋,𝑌)=\\frac{Cov(𝑋,𝑌)}{ \\sqrt{Var(𝑋)} * \\sqrt{Var(𝑌)}}$$\n",
        "<br>\n",
        "where $Cov(X,Y)$ is the covariance of $X$ and $Y$, $Var(X)$ and $Var(Y)$ are variances of $X$ and $Y$.\n",
        "<br>\n",
        "<br>\n",
        "We can use the above formula to convert between covariance and correlation matrices easily. Let's demonstrate the conversion with the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WP6Zwx8g3r46"
      },
      "outputs": [],
      "source": [
        "# Convert covariance to correlation\n",
        "def cov_to_corr(cov_matrix):\n",
        "    d = np.sqrt(np.diag(cov_matrix))\n",
        "    corr_matrix = cov_matrix / np.outer(d, d)\n",
        "    return corr_matrix\n",
        "\n",
        "# Convert correlation to covariance\n",
        "def corr_to_cov(corr_matrix, std_devs):\n",
        "    cov_matrix = corr_matrix * np.outer(std_devs, std_devs)\n",
        "    return cov_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg2lwySZ34Dg"
      },
      "outputs": [],
      "source": [
        "# Use our Apple, Ford Motor and Walmart stock returns example to demonstrate the conversion\n",
        "# Calculate standard deviations of stock returns\n",
        "stocks_returns_std = stocks_returns.std()\n",
        "\n",
        "#convert covariance to correlation example\n",
        "cov_to_corr(stock_returns_correlation_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUnU5-cx5YUg"
      },
      "outputs": [],
      "source": [
        "#convert correlation to covariance example\n",
        "corr_to_cov(stock_returns_correlation_matrix, stocks_returns_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Enc1gkMW9blk"
      },
      "source": [
        "## **2. Triangular Matrix**\n",
        "Another commonly used matrix is the triangular matrix. A triangular matrix is a square matrix where all the entries on one side of the main diagonal are zero. There are two types of triangular matrices: lower triangular matrix and upper triangular matrix.\n",
        "<br>\n",
        "<br>\n",
        "### **2.1. Lower Triangular Matrix**\n",
        "A **lower triangular matrix** is a square matrix whose entries above the main diagonal are zero. The main diagonal and entries below it can be any value. Below is a demonstration of a lower triangular matrix.\n",
        "<br>\n",
        "<br>\n",
        "$$B = \\begin{bmatrix}\n",
        "m & 0 & 0 \\\\\n",
        "a & n & 0 \\\\\n",
        "b & c & p\n",
        "\\end{bmatrix}$$\n",
        "<br>\n",
        "<br>\n",
        "### **2.2. Upper Triangular Matrix**\n",
        "\n",
        "An **upper triangular matrix** is a square matrix whose entries below the main diagonal are zero. The main diagonal and entries above it can be any value. Below is a demonstration of an upper triangular matrix.\n",
        "<br>\n",
        "<br>\n",
        "$$B = \\begin{bmatrix}\n",
        "m & a & b \\\\\n",
        "0 & n & c \\\\\n",
        "0 & 0 & p\n",
        "\\end{bmatrix}$$\n",
        "<br>\n",
        "<br>\n",
        "### **2.3. Properties of a Triangular Matrix**\n",
        "Here are some key properties of a triangular matrix:\n",
        "<br>\n",
        "<br>\n",
        "&emsp;&emsp;1. The determinant is the product of the diagonal elements.\n",
        "<br>\n",
        "&emsp;&emsp;2. The inverse of a triangular matrix (if it exists) is also triangular.\n",
        "<br>\n",
        "&emsp;&emsp;3. The product of two upper (or lower) triangular matrices is upper (or lower) triangular.\n",
        "\n",
        "The above properties are all very easy to prove using matrix operations, so we will leave these proofs to the readers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVWL_0Se-6ac"
      },
      "source": [
        "## **3. Symmetric Positive Definite (PD) Matrices and Symmetric Positive Semi-Definite (SPD) Matrices**\n",
        "In this section, we are going to introduce two important types of matrices: symmetric positive definite (PD) matrices and symmetric positive semi-definite matrices (SPD).\n",
        "<br>\n",
        "<br>\n",
        "### **3.1. Symmetric Positive Definite (PD) Matrices**\n",
        "A symmetric matrix $A$ is positive definite if $x^{T}Ax\\gt 0$ for all non-zero vectors $x$.\n",
        "<br>\n",
        "Here are some key properties of a symmetric positive definite matrix:\n",
        "<br>\n",
        "<br>\n",
        "&emsp;&emsp;1. All eigenvalues of the matrix are positive\n",
        "<br>\n",
        "&emsp;&emsp;2. The determinant of the matrix is positive\n",
        "<br>\n",
        "&emsp;&emsp;3. The matrix has full rank\n",
        "<br>\n",
        "&emsp;&emsp;4. The matrix is invertible\n",
        "<br>\n",
        "&emsp;&emsp;5. The summation and multiplication of two symmetric positive definite matrices is also a symmetric positive definite matrix\n",
        "<br>\n",
        "<br>\n",
        "These properties of symmetric positive definite matrices—full rank, positive determinant, and positive eigenvalues—are intimately connected. These characteristics make symmetric positive definite matrices particularly useful in various mathematical and applied contexts, providing guarantees of non-singularity, stability, and convergence in many algorithms and methods.\n",
        "<br>\n",
        "Usually, a quick way to identify if a symmetric matrix is positive definite is to calculate its determinant and check if the determinant is positive. For example, let's take a look at the following symmetric matrix.\n",
        "<br>\n",
        "$$\\begin{bmatrix}\n",
        "3 & 4 \\\\\n",
        " 4& 5\n",
        "\\end{bmatrix}$$\n",
        "<br>\n",
        "This is a symmetric matrix. However, the determinant is $(3\\times 5)-(4\\times 4)=-1$, this matrix is not positive definite.\n",
        "<br>\n",
        "<br>\n",
        "### **3.2. Symmetric Positive Semidefinite (SPD) Matrices**\n",
        "A symmetric matrix $A$ is positive semidefinite if $x^{T}Ax\\ge 0$ for all vectors is $x$.\n",
        "<br>\n",
        "Here are the key properties of a symmetric positive semidefinite matrix:\n",
        "<br>\n",
        "<br>\n",
        "&emsp;&emsp;1. All eigenvalues of the matrix are non-negative\n",
        "<br>\n",
        "&emsp;&emsp;2. The determinant of the matrix is non-negative\n",
        "<br>\n",
        "&emsp;&emsp;3. The matrix may not have full rank\n",
        "<br>\n",
        "&emsp;&emsp;4. The matrix is not always invertible\n",
        "<br>\n",
        "<br>\n",
        "We usually also use the determinant of a symmetric matrix to quickly check if it is also a positive semidefinite. For the following matrix example,\n",
        "<br>\n",
        "$$\\begin{bmatrix}\n",
        "1 & 1 & 1 \\\\\n",
        "1 & 1 & 1 \\\\\n",
        "1 & 1 & 1\n",
        "\\end{bmatrix}$$\n",
        "<br>\n",
        "we can check that the determinant is 0. Hence, this matrix is a positive semidefinite matrix.\n",
        "<br>\n",
        "Going back to covariance matrices and correlation matrices, they are both symmetric positive semidefinite but not necessarily symmetric positive definite. The following Python code can be used to check if a matrix is a symmetric positive definite or a symmetric positive semidefinite."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_VYt_2s_I5k"
      },
      "outputs": [],
      "source": [
        "def is_positive_definite(matrix):\n",
        "    return np.all(np.linalg.eigvals(matrix) > 0)\n",
        "\n",
        "A = stock_returns_correlation_matrix\n",
        "\n",
        "print(\"Is A positive definite?\", is_positive_definite(A))\n",
        "print(\"Eigenvalues:\", np.linalg.eigvals(A))\n",
        "print(\"Determinant:\", np.linalg.det(A))\n",
        "print(\"Rank:\", np.linalg.matrix_rank(A))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_s_z6TvDRHR"
      },
      "outputs": [],
      "source": [
        "def is_positive_semidefinite(matrix):\n",
        "    return np.all(np.linalg.eigvals(matrix) >= 0)\n",
        "\n",
        "A = stock_returns_correlation_matrix\n",
        "\n",
        "print(\"Is A positive semidefinite?\", is_positive_semidefinite(A))\n",
        "print(\"Eigenvalues:\", np.linalg.eigvals(A))\n",
        "print(\"Determinant:\", np.linalg.det(A))\n",
        "print(\"Rank:\", np.linalg.matrix_rank(A))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQti-jI2EsIu"
      },
      "source": [
        "## **4. Cholesky Factorization**\n",
        "### **4.1. Definition of Cholesky Factorization**\n",
        "**Cholesky factorization** is a method to decompose a symmetric matrix. Cholesky factorization states that a symmetric positive definite matrix $S$ can be decomposed as follows:\n",
        "<br>\n",
        "$$S = LL^{T}$$\n",
        "\n",
        "where $L$ is a lower triangular matrix or a Cholesky factor.\n",
        "<br>\n",
        "<br>\n",
        "Let's look at a numeric example. Suppose we have the following $3\\times 3$ symmetric positive definite matrix.\n",
        "<br>\n",
        "<br>\n",
        "$$S =\\begin{bmatrix}\n",
        "s_{11} & s_{21} & s_{31} \\\\\n",
        "s_{21} & s_{22} & s_{32} \\\\\n",
        "s_{31} & s_{23} & s_{33}\n",
        "\\end{bmatrix}=\\begin{bmatrix}\n",
        "l_{11} & 0 & 0 \\\\\n",
        "1_{21} & l_{22} & 0 \\\\\n",
        "l_{31} & l_{32} & l_{33}\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "l_{11} & l_{21} & l_{31} \\\\\n",
        "0 & l_{22} & l_{32} \\\\\n",
        "0 & 0 & l_{33}\n",
        "\\end{bmatrix}=LL^{T}$$\n",
        "<br>\n",
        "$$\\Downarrow $$\n",
        "<br>\n",
        "<br>\n",
        "$$LL^{T}=\\begin{bmatrix}\n",
        "l_{11}^{2} & l_{11}l_{21} & l_{11}l_{13} \\\\\n",
        "l_{21}l_{11} & l_{21}^{2} +l_{22}^{2}& l_{21}l_{31}+l_{22}l_{32} \\\\\n",
        "l_{31}l_{11} & l_{21}l_{31} + l_{22}l_{32}& l_{31}^{2}+l_{32}^{2}+l_{33}^{2}\n",
        "\\end{bmatrix}$$\n",
        "<br>\n",
        "<br>\n",
        "From the above matrices, we can get the following formulae for the lower triangular matrix.\n",
        "<br>\n",
        "$l_{11}=\\sqrt{s_{11}}$\n",
        "<br>\n",
        "$l_{21}=\\frac{s_{21}}{l_{11}}$\n",
        "<br>\n",
        "$l_{31}=\\frac{s_{31}}{l_{11}}$\n",
        "<br>\n",
        "$l_{22}=\\sqrt{s_{22}-l_{21}^{2}}$\n",
        "<br>\n",
        "$l_{32}=\\frac{s_{23}-l_{21}l_{31}}{l_{22}}$\n",
        "<br>\n",
        "$l_{33}=\\sqrt{s_{33}-l_{31}^{2}-l_{32}^{2}}$\n",
        "<br>\n",
        "<br>\n",
        "To get the above solution, you must start to solve the first row of $LL^{T}$. Then, you move on to the second row and the next row until you solve all the elements in $L$.\n",
        "<br>\n",
        "Let's look at a numeric example. Suppose we have the following symmetric positive definite matrix (please verify),\n",
        "$$S =\\begin{bmatrix}\n",
        "25 & 15 & -5 \\\\\n",
        "15 & 18 & 0 \\\\\n",
        "-5 & 0 & 11\n",
        "\\end{bmatrix}$$\n",
        "<br>\n",
        "Let's find the Cholesky factor using the above equations we derived.\n",
        "<br>\n",
        "$l_{11}=\\sqrt{25}=5$\n",
        "<br>\n",
        "$l_{21}=\\frac{15}{5}=3$\n",
        "<br>\n",
        "$l_{31}=\\frac{-5}{5}=-1$\n",
        "<br>\n",
        "$l_{22}=\\sqrt{18-9}=3$\n",
        "<br>\n",
        "$l_{32}=\\frac{0-(3)(-1)}{3}=1$\n",
        "<br>\n",
        "$l_{33}=\\sqrt{11-(-1)^{2}-(1)^{2}}=3$\n",
        "<br>\n",
        "<br>\n",
        "Hence, the Cholesky factor of $S$ is\n",
        "<br>\n",
        "<br>\n",
        "$$L_{s} =\\begin{bmatrix}\n",
        "5 & 0 & 0 \\\\\n",
        "3 & 3 & 0 \\\\\n",
        "-1 & 1 & 3\n",
        "\\end{bmatrix}$$\n",
        "<br>\n",
        "<br>\n",
        "Python Numpy has a function to calculate the Choleksy factor for a matrix. We can use it to verify our manual calculation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZKxLCVSheJN"
      },
      "outputs": [],
      "source": [
        "S= np.array([[25,15,-5],\n",
        "             [15,18,0],\n",
        "             [-5,0,11]])\n",
        "\n",
        "L_numpy = np.linalg.cholesky(S)\n",
        "print(\"\\nNumPy Cholesky factor:\")\n",
        "print(L_numpy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2y0YyPUmUeV"
      },
      "source": [
        "Choleksy factorization algorithum greatly improves the computer efficiency to solve for linear equations. It can also be used to compute the inverse of a matrix more efficiently than some other methods. Choleksy factorization is also numerically stable, making it useful for computations involving ill-conditioned matrices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70wE_5d6FlpM"
      },
      "source": [
        "### **4.2. Using Cholesky Factorization for Monte Carlo Simulation**\n",
        "Cholesky factorization is useful in Monte Carlo simulations or other optimization tasks. In this section, we will use Cholesky factorization in a Monte Carlo simulation to generate correlated random variables. Let's use the Apple and Ford Motor stock return correlation matrix as an example. The process involves the following steps:\n",
        "<br>\n",
        "<br>\n",
        "&emsp;&emsp;1. Compute the Cholesky factorization of the stock return correlation matrix to get the factor L.\n",
        "<br>\n",
        "&emsp;&emsp;2. Generate independent standard normal random variable Z.\n",
        "<br>\n",
        "&emsp;&emsp;3. Compute X = LZ to get correlated random variables.\n",
        "<br>\n",
        "<br>\n",
        "First, let's calculate the correlation of Apple and Ford Motor stock returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQAXgNRIDwsx"
      },
      "outputs": [],
      "source": [
        "# Create a new dataframe with just Apple and Ford Motor stock returns from the dataframe from the last example\n",
        "two_stock_returns = stocks_returns[[\"AAPL\", \"F\"]]\n",
        "original_correlation = two_stock_returns.corr()\n",
        "original_correlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw9ZhR4tOZuF"
      },
      "source": [
        "Now, let's implement the simulation using Cholesky factorization as described above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5dvQZlyGCPU"
      },
      "outputs": [],
      "source": [
        "# Create a function to generate random samples from correlated variables\n",
        "def generate_correlated_samples(n_samples, correlation_matrix):\n",
        "    # Compute Cholesky factorization\n",
        "    L = np.linalg.cholesky(correlation_matrix)\n",
        "\n",
        "    # Generate independent standard normal samples\n",
        "    Z = np.random.standard_normal((correlation_matrix.shape[0], n_samples))\n",
        "\n",
        "    # Generate correlated samples\n",
        "    X = L @ Z\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "# Generate samples\n",
        "n_samples = 10000\n",
        "X = generate_correlated_samples(n_samples, original_correlation)\n",
        "X = pd.DataFrame(generate_correlated_samples(n_samples, original_correlation).T, columns = [\"APPL\",\"F\"])\n",
        "# Compute sample correlation\n",
        "sample_correlation = X.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7728Ia0OyJM"
      },
      "source": [
        "Our next step is to visualize the result from original data and sampled data using a Monte Carlo simulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnD3Is0uFpmM"
      },
      "outputs": [],
      "source": [
        "# Visualize results\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "sns.heatmap(original_correlation, annot=True, cmap='coolwarm', vmin=-1, vmax=1, ax=ax1)\n",
        "ax1.set_title('Original Correlation Matrix')\n",
        "ax1.set(xlabel='', ylabel='')\n",
        "\n",
        "\n",
        "sns.heatmap(sample_correlation, annot=True, cmap='coolwarm', vmin=-1, vmax=1, ax=ax2)\n",
        "ax2.set_title('Sample Correlation Matrix')\n",
        "ax2.set(xlabel='', ylabel='')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Scatter plot of the stimulated Apple and Ford Motor stock returns\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[\"APPL\"], X[\"F\"], alpha=0.1)\n",
        "plt.title('Scatter Plot of the Simulated Apple and Ford Motor stock returns')\n",
        "plt.xlabel('Apple')\n",
        "plt.ylabel('Ford Motor')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}